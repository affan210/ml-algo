{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000, tolerance=1e-4):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.tolerance = tolerance\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def calculate_gradient(self, X, y, theta):\n",
    "        m = len(y)\n",
    "        z = np.dot(X, theta)\n",
    "        h = self.sigmoid(z)\n",
    "        gradient = np.dot(X.T, (h - y)) / m\n",
    "        return gradient\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.theta = np.zeros(n)  # Initialize weights with zeros\n",
    "        prev_cost = float('inf')  # ensures that the first iteration of will certainly lead to a decrease in the cost function.\n",
    "        \n",
    "        for iteration in range(self.num_iterations):\n",
    "            gradient = self.calculate_gradient(X, y, self.theta)\n",
    "            self.theta -= self.learning_rate * gradient\n",
    "            \n",
    "            # Calculate the cost function (log-likelihood)\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.sigmoid(z)\n",
    "            cost = -(1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "            \n",
    "            # Check for convergence based on tolerance\n",
    "            if abs(prev_cost - cost) < self.tolerance:\n",
    "                print(f\"Converged after {iteration + 1} iterations.\")\n",
    "                break\n",
    "            \n",
    "            prev_cost = cost\n",
    "    \n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.theta)\n",
    "        h = self.sigmoid(z)\n",
    "        # Use a threshold of 0.5 for binary classification\n",
    "        predictions = (h >= 0.5).astype(int)\n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        z = np.dot(X, self.theta)\n",
    "        h = self.sigmoid(z)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./datasets/heartdisease.csv\")\n",
    "df.select_dtypes(include=['object']).columns\n",
    "df.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some sample data\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(100, 2)\n",
    "y = (X[:, 0] + X[:, 1] > 0).astype(int)  # A simple linear decision boundary\n",
    "# Split the data into training and test sets (you should use a more robust method in practice)\n",
    "X_train, X_test = X[:80], X[80:]\n",
    "y_train, y_test = y[:80], y[80:]\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "model = LogisticRegression(learning_rate=0.01, num_iterations=10000, tolerance=1e-4)\n",
    "model.fit(X_train, y_train)\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y_true):\n",
    "    return np.mean(y_pred == y_true)\n",
    "\n",
    "def calculate_precision(y_pred, y_true):\n",
    "    true_positives = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    false_positives = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    return true_positives / (true_positives + false_positives)\n",
    "\n",
    "def calculate_recall(y_pred, y_true):\n",
    "    true_positives = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    false_negatives = np.sum((y_pred == 0) & (y_true == 1))\n",
    "    return true_positives / (true_positives + false_negatives)\n",
    "\n",
    "def calculate_f1_score(y_pred, y_true):\n",
    "    precision = calculate_precision(y_pred, y_true)\n",
    "    recall = calculate_recall(y_pred, y_true)\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def calculate_metric(metric_name, y_pred, y_true):\n",
    "    if metric_name == \"accuracy\":\n",
    "        return calculate_accuracy(y_pred, y_true)\n",
    "    elif metric_name == \"precision\":\n",
    "        return calculate_precision(y_pred, y_true)\n",
    "    elif metric_name == \"recall\":\n",
    "        return calculate_recall(y_pred, y_true)\n",
    "    elif metric_name == \"f1_score\":\n",
    "        return calculate_f1_score(y_pred, y_true)\n",
    "    elif metric_name == \"list_metrics\":\n",
    "        return [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid metric name. Available metrics: {', '.join(calculate_metric('list_metrics', None, None))}\")\n",
    "\n",
    "def calculate_metrics(y_pred, y_true):\n",
    "    metrics = {}\n",
    "    metrics[\"accuracy\"] = calculate_accuracy(y_pred, y_true)\n",
    "    metrics[\"precision\"] = calculate_precision(y_pred, y_true)\n",
    "    metrics[\"recall\"] = calculate_recall(y_pred, y_true)\n",
    "    metrics[\"f1_score\"] = calculate_f1_score(y_pred, y_true)\n",
    "    return metrics\n",
    "\n",
    "def calculate_roc_auc(y_prob, y_true, plot=False):\n",
    "    thresholds = np.linspace(0, 1, 100)  # Threshold values\n",
    "    tpr_list = []  # True Positive Rate (Sensitivity)\n",
    "    fpr_list = []  # False Positive Rate\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred_thresholded = (y_prob >= threshold).astype(int)\n",
    "        true_positives = np.sum((y_pred_thresholded == 1) & (y_true == 1))\n",
    "        false_positives = np.sum((y_pred_thresholded == 1) & (y_true == 0))\n",
    "        true_negatives = np.sum((y_pred_thresholded == 0) & (y_true == 0))\n",
    "        false_negatives = np.sum((y_pred_thresholded == 0) & (y_true == 1))\n",
    "\n",
    "        tpr = true_positives / (true_positives + false_negatives)\n",
    "        fpr = false_positives / (false_positives + true_negatives)\n",
    "\n",
    "        tpr_list.append(tpr)\n",
    "        fpr_list.append(fpr)\n",
    "\n",
    "    auc = calculate_auc(tpr_list, fpr_list)\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr_list, tpr_list, linestyle='-', marker='.')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curve (AUC: {auc:.2f})')  # Include AUC score in the title\n",
    "        plt.grid(True)\n",
    "        # Add AUC score as text annotation on the plot\n",
    "        plt.annotate(f'AUC = {auc:.2f}', xy=(0.6, 0.4), xytext=(0.6, 0.7),\n",
    "                    #  arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "                     fontsize=12, color='black', backgroundcolor='white')\n",
    "        plt.show()\n",
    "\n",
    "    return auc\n",
    "\n",
    "def calculate_auc(tpr, fpr):\n",
    "    auc = 0.0\n",
    "    for i in range(1, len(tpr)):\n",
    "        width = fpr[i] - fpr[i - 1]\n",
    "        height_avg = (tpr[i] + tpr[i - 1]) / 2\n",
    "        auc += width * height_avg\n",
    "    return auc\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # ... (training and prediction code)\n",
    "\n",
    "    # Calculate and print the requested metric or list available metrics\n",
    "    metric_name = \"list_metrics\"  # Change this to the metric you want to calculate or \"list_metrics\"\n",
    "    \n",
    "    if metric_name == \"list_metrics\":\n",
    "        print(\"Available metrics:\", calculate_metric(metric_name, None, None))\n",
    "    else:\n",
    "        metric_value = calculate_metric(metric_name, y_pred, y_test)\n",
    "        print(f\"{metric_name.capitalize()}: {metric_value*100:.2f}%\")\n",
    "\n",
    "    # Calculate and print a specific metric\n",
    "    metric_name = \"accuracy\"  # Change this to the metric you want\n",
    "    metric_value = calculate_metric(metric_name, y_pred, y_test)\n",
    "    print(f\"{metric_name.capitalize()}: {metric_value*100:.2f}%\")\n",
    "    \n",
    "    # Calculate and print all eligible metrics\n",
    "    calculated_metrics = calculate_metrics(y_pred, y_test)\n",
    "    for metric_name, metric_value in calculated_metrics.items():\n",
    "        print(f\"{metric_name.capitalize()}: {metric_value*100:.2f}%\")\n",
    "\n",
    "    # Calculate and print ROC AUC score\n",
    "    auc_score = calculate_roc_auc(y_prob, y_test, plot=True)\n",
    "    print(f\"ROC AUC Score: {auc_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-algo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
